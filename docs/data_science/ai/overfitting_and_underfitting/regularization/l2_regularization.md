# L2 regularization (Ridge)

## Description

Ridge regression, a linear regression method applicable to feature selection, closely resembles ordinary least squares regression but introduces a penalty term to the cost function to counter overfitting.

In ridge regression, the cost function undergoes modification with the inclusion of a penalty term directly proportional to the square of the coefficientsâ€™ magnitude. This penalty term is regulated by a hyperparameter, often denoted as Î» or Î± dictating the regularization strength. When Î± is set to zero, ridge regression reverts to ordinary least squares regression.

The penalty termâ€™s impact manifests in shrinking the coefficientsâ€™ magnitude toward zero. This proves beneficial in mitigating overfitting, discouraging the model from excessively relying on any single feature. In effect, the penalty term acts as a form of feature selection by reducing the importance of less relevant features.

Ridge regression can be used for feature selection by examining the magnitudes of the coefficients produced by the model. Features with coefficients that are close to zero or smaller are considered less important and can be dropped from the model. The value of Î± can be tuned using cross-validation to find the optimal balance between model complexity and accuracy.

One of the main advantages of ridge regression is its ability to handle multicollinearity, which occurs when there are strong correlations between the independent variables. In such cases, ordinary least squares regression can produce unstable and unreliable coefficient estimates, but ridge regression can help stabilize the estimates and improve the overall performance of the model.

## Formula

The equation for the ridge regression loss function is as follows:

<img src="image1.png" style="width:2.26563in" />

Here, we have the following:

- N is the number of samples in the training set.
- y is the column vector of target values of size N.
- X is the design matrix of input features.
- w is the vector of regression coefficients to be estimated.
- Î± is the regularization parameter that controls the strength of the penalty term. It is a hyperparameter that needs to be tuned.

The first term in the loss function measures the mean squared error between the predicted values and the true values. The second term is the ğ“ 2 penalty term that shrinks the coefficients toward zero. The ridge regression algorithm finds the values of the regression coefficients that minimize this loss function. By tuning the regularization parameter, Î±, we can control the bias-variance trade-off of the model, with higher alpha values leading to more regularization and lower overfitting.

## How It Works

<img src="image4.png" style="width:3.5293in" />

<span dir="rtl">ØªÙˆ ØªØµÙˆÛŒØ± Ø¨Ø§Ù„Ø§ Ù†Ø´ÙˆÙ† Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ú©Ù‡ Ø¨Ø§ Ø§Ø¹Ù…Ø§Ù„ Ø¶Ø±ÛŒØ¨ Ù‡Ø§ÛŒ (</span><span dir="ltr">w</span><span dir="rtl">) Ú©ÙˆÚ†Ú© Ø§Ù…ØªØ­Ø§Ù† Ù…ÛŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø¢ÛŒØ§ Ø­Ø°Ù ÛŒÚ© Ù…ØªØºÛŒØ± Ø¯Ø§Ø±Ø§ÛŒ ØªÙˆØ§Ù† Ø¨Ù‡ Ø­Ù„ Ù…Ø´Ú©Ù„ Ù…Ø§ Ú©Ù…Ú© Ù…ÛŒÚ©Ù†Ø¯ ÛŒØ§ Ø®ÛŒØ±ØŒ Ø§Ø² Ø¢Ù†Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ±</span> <span dir="ltr">w</span> <span dir="rtl">Ùˆ</span> <span dir="ltr">b</span> <span dir="rtl">Ø±Ø§ Ø³ÛŒØ³ØªÙ… Ø¨Ø·ÙˆØ± Ø§ØªÙˆÙ…Ø§ØªÛŒÚ© ØªØ¹ÛŒÛŒÙ† Ù…ÛŒÚ©Ù†Ø¯ Ùˆ Ù…Ø§ Ø¯Ø± Ø¢Ù†Ù‡Ø§ Ù†Ù‚Ø´ÛŒ Ù†Ø¯Ø§Ø±ÛŒÙ… Ù„Ø°Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ´ Ø²ÛŒØ±ØŒ Ø³ÛŒØ³ØªÙ… Ø±Ø§ Ù…Ø¬Ø¨ÙˆØ± Ø¨Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ Ø§Ø¹Ø¯Ø§Ø¯ Ú©ÙˆÚ†Ú© Ø¨Ø±Ø§ÛŒ Ø¶Ø±ÛŒØ¨ Ù‡Ø§ÛŒ (</span><span dir="ltr">w</span><span dir="rtl">) Ø¯Ù„Ø®ÙˆØ§Ù‡ Ø®ÙˆØ¯ Ù…ÛŒÚ©Ù†ÛŒÙ…:</span>

<img src="image3.png" style="width:3.63959in" />

<span dir="ltr"></span> <span dir="rtl">Ø¯Ø± ØªØµÙˆÛŒØ± Ø¨Ø§Ù„Ø§ Ù…Ø§ ØªÙ†Ù‡Ø§ ØªØ§Ø«ÛŒØ± Ø¯Ùˆ Ù…ØªØºÛŒØ±</span> <span dir="ltr">w</span> <span dir="rtl">Ø³Ù‡ Ùˆ</span> <span dir="ltr">w</span> <span dir="rtl">Ú†Ù‡Ø§Ø± Ø±Ø§ Ú©Ù… Ú©Ø±Ø¯ÛŒÙ…ØŒ Ø§Ù…Ø§ Ø¯Ø± Ø³ÛŒØ³ØªÙ… Ù‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ ØªØ± Ø¨Ø¬Ø§ÛŒ Ø§ÛŒÙ† Ú©Ø§Ø± Ø§Ø² Ø§Ù„Ú¯ÙˆÛŒ Ø²ÛŒØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒ Ø´ÙˆØ¯:</span>

<img src="image2.png" style="width:3.42707in" />

<span dir="rtl">Ø¯Ø± Ø§Ù„Ú¯ÙˆÛŒ Ø¨Ø§Ù„Ø§</span> <span dir="ltr">n</span> <span dir="rtl">ØªØ¹Ø¯Ø§Ø¯ Ù…ØªØºÛŒØ± Ù‡Ø§ÛŒ (</span><span dir="ltr">w</span><span dir="rtl">) Ù‡Ø§ÛŒ Ø­Ø§Ø¶Ø± Ø¯Ø± Ù…Ø¹Ø§Ø¯Ù„Ù‡ Ù‡Ø³ØªØ´ØŒ Ø¨Ø§ Ø¨Ø§Ù„Ø§ Ø¨Ø±Ø¯Ù†</span> <span dir="ltr">Î»</span> <span dir="rtl">Ø³ÛŒØ³ØªÙ… Ù…Ø¬Ø¨ÙˆØ± Ø¨Ù‡ Ø§Ù†ØªØ®Ø§Ø¨</span> <span dir="ltr">w</span> <span dir="rtl">Ù‡Ø§ÛŒ Ú©ÙˆÚ†Ú©ØªØ± Ù…ÛŒ Ø´ÙˆØ¯ Ú©Ù‡ Ø¯Ù‚ÛŒÙ‚Ø§ Ù‡Ø¯Ù Ù…Ø§ Ø§Ø² Ø§Ù†Ø¬Ø§Ù…</span> <span dir="ltr">Regularization</span> <span dir="rtl">Ù‡Ø³ØªØ´</span>

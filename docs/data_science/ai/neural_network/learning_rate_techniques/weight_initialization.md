# Weight Initialization

## Description

<span dir="rtl">با استفاده از تکنیک های زیر میتونیم یه عدد اولیه خوب برای w انتخاب کنیم که از exploding/vanishing تو deep neural network جلوگیری کنه.</span>

For RELU activation function:

<img src="image2.jpg" style="width:3.29699in" />

For tanh activation function :

<img src="image3.jpg" style="width:3.25989in" />

Another commonly used heuristic is:

<img src="image1.jpg" style="width:3.2413in" />
